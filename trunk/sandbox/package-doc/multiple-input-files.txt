================================
 Docutils: Multiple Input Files
================================

:Author: Lea Wiemann <LeWiemann@gmail.com>
:Date: $Date$
:Revision: $Revision$
:Copyright: This document has been placed in the public domain.

.. sectnum::
.. contents::


Introduction
============

We would like to support documents whose source text comes from
multiple files.  For instance, the Docutils documentation tree could
be considered a large document; parsing all files into one single
document tree would enable us to do cross-linking between parts of the
documentation (our current way to cross-link between files is to link
to HTML files and fragments, which is a hack).

Note that this issue is separate from output to multiple files; after
implementing support for multiple input files, all we will be able to
do is to generate a huge single output file.

This is a collection of notes and semi-random thoughts (many of which
are credit to David, from IM conversations).  Feel free to add yours
and/or make changes as you see fit!

You can also discuss this proposal on the Docutils-develop_ mailing
list, or reach us individually via email or Jabber/Google Talk at
LeWiemann@gmail.com and dgoodger@gmail.com, respectively.

.. _Docutils-develop:
   http://docutils.sf.net/docs/user/mailing-lists.html#docutils-develop


Terminology
===========

Right now, we are using the following terminology: The whole document
is simply called *the document*.  Its input consists of multiple
*files*: A *master* file (which references the sub-documents), and
*sub-document* files.  The sub-document files should (probably) each
be processable stand-alone (without the other files), each forming a
document on its own.

Should we say "book" instead of "document"?  Is "file" confusing
(physical vs. logical entity) -- should we use "sub-document" instead?


The ``subdocument`` Directive
=============================

* The "include" directive is not usable for this because we want to
  have independent parsing contexts (for instance, section title
  adornment should not have to be consistent across input files).

* So create a "subdocument" directive (syntax: ".. subdocument::
  file.txt").  This directive causes the referenced file to be parsed
  and its document tree to be inserted in place.

  - The sub-document must have a document title.  (This document title
    will become a section title.)
    
  - The subdocument directive should be treated like a section; that
    is, no elements except for more sections or transitions may
    follow.
    
  - In order to facilitate assembling a large number of hierarchical
    files into a large document, the subdocument directive should
    allow specifying any number of files, like this::

        .. subdocument::

           chapter1.txt
               chapter1-section1.txt
               chapter1-section2.txt
           chapter2.txt
               chapter2-section1.txt
               ...

    Specifying an indented file (like chapter1-section1.txt) is
    equivalent to inserting ".. subdocument:: chapter1-section1.txt"
    at the end of chapter1.txt.

    Lists of files should be required to be directive content, not
    parameters, because file lists as parameters would be prone to
    uncaught user errors.  In this example, the indentation of
    "chapter1-section1.txt" would be stripped by the directive parser,
    which is contrary to what the user expects::

        .. subdocument:: chapter1.txt
               chapter1-section1.txt

* What to do with docinfos in subdocuments:

  - Allow for section infos by generalizing the existing docinfo node.

  - Add an option to either strip or leave docinfos.  Perhaps
    specifiable as an option to the "subdocument" directive, and/or on
    a per-input-file basis (how?), and/or as a command line option
    (applied in which order?).

  - There is currently no way to have per-section "section infos" in
    reStructuredText files, as opposed to only file-wide docinfos.  So
    the only way to get section infos in a document is to use
    sub-documents.  This might be just fine though.

  - For a first implementation, just go the easy route and strip all
    docinfos in sub-documents.

* In order to facilitate multi-file output that parallels the input
  file structure, add "source" attributes to section nodes for
  sections that come from different input files.

* Allow multiple top-level sections in sub-documents.  (No reason not
  to; the implementation doesn't get much harder.)  Sub-documents can
  therefore contain either a lone document title (and optionally a
  docinfo), or multiple top-level sections.  Only support per-document
  docinfos -- if a sub-document contains multiple top-level sections,
  don't touch field lists at all.

* Restriction: Do not allow sub-documents without a top-level section,
  or with body elements in front of the first section.  IOW, the
  sub-document may only contain PreBibliographic elements, sections,
  and transitions.  The PreBibliographic elements in front of the
  first section get moved into the section.

  David says this restriction is bad and we probably shouldn't have it
  -- for instance you might want to have an introductory paragraph in
  front of the first section of the first sub-document; Lea doesn't
  mind the restriction and says you could use the "include" directive.
  Since having the restriction makes the implementation somewhat
  easier, we agreed on having this restriction, waiting until the
  first user presents a good use case to remove it, and calling it a
  YAGNI until then.

* Silently drop header and footer in sub-documents.  (Document this in
  directives.txt though.)

* To do: Explore alternatives besides "subdocument" for the directive
  name.

* You may want to read some insightful remarks by Joaquim Baptista on
  how `files should be expected to be part of different documents`__.

  .. _different documents:
  __ http://article.gmane.org/gmane.text.docutils.devel/4043


.. _xrefs:

Cross-references
================

.. note:: You may need to read the `reST spec`_ in order to understand
   the terminology (targets, references).  In this section, "*external
   reference*" means a reference whose target is outside of the
   current file, but within the current document.

   .. _reST spec: http://docutils.sf.net/docs/ref/rst/restructuredtext.html

A major issue to think about is how to do **cross-references**
(colloquially known as **xrefs**) between files.  Things like
substitutions or role definitions should not be shared between files
(their definitions can simply be loaded using the "include"
directive).  However, sharing targets and thereby allowing
cross-references between files is one of the major points why we want
an architecture that supports multiple input files.

Issues arise once we think about how to group target names into
namespaces.  Unfortunately, simply putting all targets into a global,
document-wide namespace is bound to cause collisions; files that were
processable stand-alone are no longer processable when used in
conjunction with other files because they share common target names.


Bad Proposal: Local and Global Namespace, no Qualifiers
-------------------------------------------------------

An obvious solution would be to add a notion of a file-local and a
global namespace.  When trying to resolve a reference, first the
target name is looked up in the local namespace of the current file;
if no suitable target is found there, the target name is searched for
document-wide, in the global namespace; if the target name exists and
is unique within the document, the reference can be resolved.

.. sidebar:: Why independent references are a good idea

   While the requirement that the whole document be processed in order
   to resolve external references makes implementation easier, it is
   certainly worthwhile to provide for a means to resolve external
   references without a re-run of the whole document for speed
   reasons:

   Since authoring can involve an edit-process-edit-process cycle, it
   should be possible to process files individually, rather than the
   whole document (which can be very slow).  Of course, as long as
   external references are marked in the source file as such, they
   can, in a stand-alone pass, always be marked as "unresolvable"
   (e.g. in red) in the output, and only be resolved when the whole
   document is processed.  However, it would be even better to be able
   to actually resolve the references.

If references to the global namespace are not marked up as such,
however, the individual files are no longer processable stand-alone
because they contain unresolvable references.  While it may be
acceptable that external cross-references do not (fully) work any
longer when a file is processed stand-alone, it would be nice to be
able to handle unresolved external references somehow (at least by
marking them as "unresolvable" in the output), rather than simply
throwing an error (see the sidebar).

This can be solved by marking external references as such, like this::

    `local target name`_
    `-> global target name`_

where "local target name" must be a unique target name within the
current file, and "global target name" must be a unique target name
within the current document.

.. We would need to explicitly establish a notion of "stand-alone" vs.
   "full document" processing in this case, but this proposal is being
   shot down anyway, so I'm not going to get into it.


Drawbacks
~~~~~~~~~

This approach turns out to have a major drawback though: External
references depend on the context of the containing document.  However,
as Joaquim `pointed out`__, files should be expected to be part of
several documents.  This means that once a file is put into the
context of a new document, its external references might point to
non-existing or duplicate targets.  This seems like a maintenance
problem for complex (large) collections of documentation.

__ different documents_

Another peculiarity of this system is that an external reference is
not associated to the file that defines the target outside of a
document context.  This brings the advantage that renaming and moving
files won't invalidate reference names.  On the downside, it lacks
clarity for the reader because the file containing a target is often
not inferable from the target name (try to guess which file ```->
html4css1`_`` links to) -- this may be significant since
reStructuredText should be readable in its source form.


Proposal: Explicit Qualifiers
-----------------------------

Since linking to targets outside the scope of the current document
appears to be a bad idea, we may need some form of qualifiers.


Namespace Identifiers
~~~~~~~~~~~~~~~~~~~~~

This makes it necessary to add a notion of *namespace identifiers*.

.. sidebar:: Why headers are a bad idea

   One of the appealing features of reStructuredText, compared to
   LaTeX, is that writing a document does not require writing a
   header.  Just type the title, some text, run rst2html, and you're
   done.  Writing a stand-alone LaTeX document on the other hand
   typically begins with declaring the \\documentclass, loading all
   the packages you need for your document, setting some options, and
   finally \\begin{document}.

   While it may not be possible to go *entirely* without any explicit
   markup, it is certainly a worthwhile goal to keep the amount of
   such markup to a minimum.

It is possible to always name the namespace of the current file (as it
is done in C++).  For instance, "``.. namespace:: frob``" at the
beginning of a file could declare that the namespace of the current
file is called "frob".  However, this is a little verbose as it adds a
line at the top of each file (see the sidebar).  Also, it removes the
reader's ability to easily look up the referenced files (you might not
know which file(s) declare the "frob" namespace).

On the other hand, namespace names could also be derived from paths
and file names.  (Note though that these two options need not be
mutually exclusive.)  Since using only the file name would cause
ambiguity, it is necessary to include its path in the namespace name.
For instance, the file ``docs/dev/todo.txt`` could be referenced by
the implicit namespace identifier ``docs/dev/todo.txt``; a reference
would look like ```docs/dev/todo.txt -> large documents`_`` (see
`Qualifier Syntax`_ below though).  Using paths relative to the
current file makes it hard to move files or document parts.
Therefore, we need to establish the notion of a *project root* which
path names are relative to:


Project Root
~~~~~~~~~~~~

Probably, the project root should be specified using docutils.conf.
One option is to place a docutils.conf in the project root with the
following lines::

    [general]
    is-project-root: yes

When the reference resolver needs to locate a file, it checks for a
docutils.conf setting the is-project-root switch first in the current
directory, then in the parent directory, and so on.

Having an ``is-project-root`` switch has the advantage that it is very
simple, and it only requires one docutils.conf file in the project
root.  (However, the ``is-project-root`` does not integrate well
conceptually with the existing settings system, since it says
something about the containing directory, not about how to process the
files contained therein.)

It may also be desirable to specify alternate project roots; in this
case, looking up files is done by checking for their existence first
in the main project root, and then in the alternate roots.  A use-case
for the Docutils tree would be to have the main Docutils directory as
the main project root, and its parent directory (for referencing files
in the web/ and sandbox/ directories) and the docs/ subdirectory (for
convenience) as alternate roots.  In this example, the docutils.conf
in the main Docutils directory might look like this::

    [general]
    is-project-root: yes
    alternate-project-root: ../
    alternate-project-root: docs/

``alternate-project-root`` settings are additive, and they are honored
by the reference resolver for all docutils.conf files in the directory
of the current file and all ancestor directories up to project root
directory (which is declared by ``is-project-root: yes``).  The
disadvantages of the ``alternate-project-root`` are that it may be too
complicated, it may cause ambiguities when resolving namespace
identifiers, and it means that the referenced file is not as easily
locatable by the reader of a reStructuredText file.


Namespace Aliases
~~~~~~~~~~~~~~~~~

A general disadvantage of using paths as namespace identifiers is that
changes in the directory structure cause a massive amount of changes
in the reStructuredText files, because all the paths need to be
updated.  This is not any worse than the current situation.  However,
to improve maintainability it would be desirable to make the namespace
of an often-referenced files known under a shorter name.  (This
mechanism should be file-local; that is, the shorter namespace
identifier should only be valid within the file where it is declared.)
For instance, one could make "docs/ref/restructuredtext.txt" known as
"spec" using one of the following syntax alternatives::

    .. namespace:: spec = docs/ref/restructuredtext.txt

    .. namespace:: spec <- docs/ref/restructuredtext.txt
            (like smalltalk)

    .. namespace:: docs/ref/restructuredtext.txt = spec
            (unusual assignment order)

    .. namespace:: docs/ref/restructuredtext.txt -> spec
            (possible confusion with qualifier syntax)

    .. namespace:: docs/ref/restructuredtext.txt == spec
    .. namespace:: spec == docs/ref/restructuredtext.txt
            (both are equivalent)

An alternatives to "namespace" is "alias", but it is not clear what
exactly "alias" aliases.


Importing Namespaces
~~~~~~~~~~~~~~~~~~~~

While namespaces should generally be available without explicitly
importing them (in order to avoid length headers), it would probably
be handy to have a means of inserting all targets of another namespace
into the current one.  Contenders for the syntax::

    .. import:: namespace   (Pythonic)

    .. import-targets:: namespace   (more verbose)

    .. using:: namespace    (like C++)

Or, provided that we use "``.. namespace:: short-name <- namespace``",
and "```namespace -> target`_``" as reference syntax, this would be a
logical fit::

    .. namespace:: <- namespace
    `-> target`_                   (instead of `namespace -> target`_)


Qualifier Syntax
~~~~~~~~~~~~~~~~

Note that in all of the examples below, the reference text of the
inline reference (like ```<namespace> target`_``) comes out as
"target", i.e. the namespace identifier and the decoration or
separator are stripped.

* Angled brackets::

      `<namespace> target`_
      .. _reference: <namespace> target_

  This is similar to the syntax for embedded URI's (```target
  <URI>`_``).  It fits well into the existing syntax, but may be
  confusing at the same time.

* Arrows::

      `namespace -> target`_
      .. _reference: namespace -> target_

  In case of "imported" namespaces (analogously to C++'s "using
  namespace" keyword), this would also allow for namespace-less
  references; for instance, after ".. import:: namespace" it would be
  possible to write::

      `-> target`_

  The syntax may collide with existing target names.  (We could give
  existing targets name priority though.  I.e., the reference resolver
  checks whether "foo -> bar" is a valid local target name before
  looking up target "bar" in namespace "foo".)

  Alternatives for the arrow: ``-->``, or ``>>``.

  Should there we whitespace around the arrow (yes/no/optional)?  It
  looks more readable with whitespace, but omitting the whitespace
  makes it terser.

* HTML fragment identifier syntax::

      `namespace#target`_
      .. _reference: namespace#target_

  This is probably a bad idea since it easily gets confused with
  relative URIs::

      .. _reference1: spec.html#target  (<--- URI)
      .. _reference2: spec#target_      (<--- external target)

  This is especially bothersome in cases such as Docutils' own
  documentation tree, where we would want to refactor targets
  referring to relative URI's into indirect targets cross-referencing
  targets in other documents (so ``config.html#foo`` becomes
  ``config#foo_``); visually distinguishing between old-style and
  new-style targets is important, and a qualifier syntax paralleling
  HTML's fragment identifier makes this hard.


Caching
-------

In order to be able to regenerate the whole documentation tree in a
timely manner after changing a single file, it is necessary to
implement a caching system.

Processing a document is done in the following steps:

1. For each file in the documentation tree, parse it and turn the
   target names into file-local ID's (this includes error handling for
   duplicate target names).  Cache the parse tree, the name-to-ID
   mapping, and the list of all included files.  Skip this step for
   files whose cache entry date-stamp is newer than the file's mtime
   and ctime, and all included files mtimes and ctimes.

   This means that the either the "subdocument" directive must be
   resolved at transform time, because otherwise we cannot store the
   doctree before the sub-document has been inserted.

2. For each file, run transforms, resolving external references using
   the cached name-to-ID mappings of other files.

3. Write out the resulting document (currently a single file).  (The
   writer needs to turn namespace/ID pairs into output-file-local
   ID's.)

Processing a file stand-alone is done in the same way, except that
steps 1 and 2 are only performed for the file being processed, not for
each file in the documentation tree.  If other files' cached
name-to-ID mappings are not up-to-date (when being accessed in step
2), they should be automatically updated.

All cache entries should be stored in a project-wide cache file, in
order to avoid LaTeX-like creation of many junk files.  Possible names
include docutils.cache, docutils.aux, or either of them with leading
dot (does this work reliably on Windows?).  The file is stored in the
project root and contains a header and a large pickle string (reading
and writing even large strings of pickled data is reasonably fast).
In the header of the cache file, store sys.version and
docutils.__version__; discard cache files that have the wrong version.

Potential security issue: Since unpickling is unsafe, an attacker
could provide a carefully crafted cache file, which is then
automatically picked up by Docutils.  Remedies: Insert some
unguessable system-specific key (generate randomly and store in
~/.docutils.cache.key), and automatically discard cache files that
have the wrong key.  Or simply place a big warning in the
documentation not to accept cache files from strangers.

No caching is done if no project-root is found (which means that the
file being processed is not part of a larger documentation tree).


Implementation
--------------

As described in section Caching_, when processing files stand-alone
and resolving their external references, it may be necessary to
process (or re-process) referenced files.  Since this is during
transform-time, the parser instance is no longer available; it is
therefore necessary to create a new instance.

We might want to choose a parser.  However, it seems a lot easier to
assume only standard reStructuredText files for now and simply
hard-code the standard reStructuredText parser.

All requests for doctree and name-to-ID mappings should go through the
caching system.  In case of a miss, the caching system instantiates a
parser and (re-)parses the requested file.

In fact, all calls by the standalone reader [#reader]_ to the
reStructuredText parser should go through the cache.  In the case of
independent files which are not part of a larger documentation tree,
the system simply does not find a project root and always assumes a
cache miss.

.. [#reader] Or should we create a new reader?
